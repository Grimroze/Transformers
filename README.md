# Transformers
Transformer from Scratch This project implements the Transformer model architecture from scratch using TensorFlow, based on the seminal paper "Attention Is All You Need". It covers building all core components, including:  Scaled Dot Product Self-Attention
